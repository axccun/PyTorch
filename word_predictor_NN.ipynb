{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5Wz9bWh7fO6xZCtMMss3J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/axccun/PyTorch/blob/main/word_predictor_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySCKPvY_QhAE",
        "outputId": "d8459a62-d7b3-45ab-bc43-f55179db00ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "M5uM1KjRW-Ox"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = \"\"\"In the land of Vrindavan, a child named Krishna was born under the dark sky of a stormy night. He was not an ordinary child, but an incarnation of the Supreme Being. Devaki and Vasudeva were his earthly parents. When he was born, the prison gates opened mysteriously, and the guards fell asleep.\n",
        "\n",
        "Vasudeva carried the newborn Krishna across the Yamuna river to Gokul. The river rose high, but then parted, allowing him to cross safely. In Gokul, Krishna was raised by Yashoda and Nanda.\n",
        "\n",
        "Krishna grew up performing miracles. As a child, he killed demons like Putana, who came disguised as a nurse. He also lifted the Govardhan Hill on his little finger to protect the villagers from Indra’s wrath.\n",
        "\n",
        "His flute enchanted everyone in Vrindavan. The gopis would leave their homes upon hearing it. Among them, Radha was his eternal beloved. Their love was divine, beyond the physical realm.\n",
        "\n",
        "Krishna taught dharma through his actions. In the Mahabharata, he served as Arjuna’s charioteer. When Arjuna hesitated to fight his own kin, Krishna revealed the Bhagavad Gita.\n",
        "\n",
        "Krishna said, “You have the right to perform your duty, but not to the fruits of action. Be steadfast in yoga, perform your duty without attachment.”\n",
        "\n",
        "He also said, “Whenever there is a decline in righteousness and an increase in unrighteousness, I manifest myself.”\n",
        "\n",
        "Krishna played many roles—cowherd, prince, diplomat, warrior, philosopher, and friend. Yet, he remained unattached, performing his duties without ego.\n",
        "\n",
        "Even in war, Krishna did not pick up a weapon. He upheld his vow but guided Arjuna with wisdom. He showed his universal form, terrifying and infinite, containing all gods and time itself.\n",
        "\n",
        "Krishna’s leelas (divine play) have inspired devotion for centuries. His childhood mischiefs, butter thefts, and dance with the gopis are sung in bhajans and celebrated in temples across India.\n",
        "\n",
        "In Dwarka, he ruled as a wise king. He married Rukmini, Satyabhama, and others, fulfilling many roles but always devoted to dharma.\n",
        "\n",
        "His departure from the world marked the end of Dvapara Yuga. Yet, Krishna lives on in every heart that chants his name.\n",
        "\n",
        "“Hare Krishna Hare Krishna, Krishna Krishna Hare Hare. Hare Rama Hare Rama, Rama Rama Hare Hare.”\n",
        "\n",
        "Krishna is within and without. He is the light in all beings, the intelligence in the intelligent, the strength of the strong devoid of desire and attachment.\n",
        "\n",
        "Surrender unto me alone, and I shall liberate you from all sins. Do not fear.\n",
        "\n",
        "He is the Parthasarathi, the guide of Arjuna, the slayer of Kamsa, the protector of dharma.\n",
        "\n",
        "The peacock feather on his head, the flute in his hands, and the smile on his lips reveal the joy of divine love.\n",
        "\n",
        "To meditate upon Krishna is to connect with the timeless truth. He is unborn, eternal, and blissful.\n",
        "\n",
        "Let the next word always be Krishna, for where Krishna is, there is truth, beauty, and love.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VycAx3aTXnDn"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkUa8rlGXuxV",
        "outputId": "a4ecb267-0f29-4f71-8521-3db888b44df9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize\n",
        "tokens = word_tokenize(document.lower())"
      ],
      "metadata": {
        "id": "7PbFWTOIX7zb"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {'<UNK>':0}\n",
        "for token in Counter(tokens).keys():\n",
        "  if token not in vocab:\n",
        "    vocab[token]=len(vocab)"
      ],
      "metadata": {
        "id": "2j0I7NaIYQrq"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAEqbeX4Yzv8",
        "outputId": "b85df572-0979-49f0-886b-b43d5ce15954"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "272"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extract sentences from data\n",
        "input_sentences = document.split('.')"
      ],
      "metadata": {
        "id": "h5cs7iVQY2IV"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(text,vocab):\n",
        "  numerical_sentence = []\n",
        "  for token in text:\n",
        "    if token in vocab:\n",
        "      numerical_sentence.append(vocab[token])\n",
        "    else:\n",
        "      numerical_sentence.append(vocab['<UNK>'])\n",
        "  return numerical_sentence"
      ],
      "metadata": {
        "id": "yRgZh23dZZZW"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_input_sentences = []\n",
        "for sentence in input_sentences:\n",
        "  numerical_input_sentences.append(text_to_indices(word_tokenize(sentence.lower()),vocab))"
      ],
      "metadata": {
        "id": "nd3ld8P6ZL1m"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " len(numerical_input_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vDq-LAEaKYj",
        "outputId": "e9e1f8b7-3f78-4634-9216-d56478c30e34"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  training_sequence = []\n",
        "  for sentence in numerical_input_sentences:\n",
        "    for _ in range(1,len(sentence)):\n",
        "      training_sequence.append(sentence[:_+1])"
      ],
      "metadata": {
        "id": "GJ12jUG9awRn"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(training_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjvwy8Zt1Ych",
        "outputId": "cb292a80-2863-4088-903b-f142d5fb47bb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "501"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_list = []\n",
        "for sequence in training_sequence:\n",
        "  len_list.append(max(sequence))\n",
        "max(len_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU-AxLGU1rui",
        "outputId": "8ca8d9b0-3f67-4606-e410-6191567e47bf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "271"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padding_training_sequence = []\n",
        "for sentence in training_sequence:\n",
        "  padding_training_sequence.append([0]*(max(len_list) - len(sentence) )+ sentence)"
      ],
      "metadata": {
        "id": "MhZ1DxNs2AqA"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padding_training_sequence[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XNuMt552fgU",
        "outputId": "ae6e16b3-ca49-4fce-e075-853a80d653fa"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 2]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(padding_training_sequence[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDoqDn4V2kSk",
        "outputId": "fef7a368-480e-42ad-c2b8-1cd95f96e1cd"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "271"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padding_training_sequence = torch.tensor(padding_training_sequence,dtype = torch.long)"
      ],
      "metadata": {
        "id": "-CTWn3hK2okZ"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padding_training_sequence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZZ3clal22Wz",
        "outputId": "f415d338-39e5-404e-b585-542157477028"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0,   0,   0,  ...,   0,   1,   2],\n",
              "        [  0,   0,   0,  ...,   1,   2,   3],\n",
              "        [  0,   0,   0,  ...,   2,   3,   4],\n",
              "        ...,\n",
              "        [  0,   0,   0,  ...,   6, 271,   6],\n",
              "        [  0,   0,   0,  ..., 271,   6,  28],\n",
              "        [  0,   0,   0,  ...,   6,  28, 104]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = padding_training_sequence[:,:-1]\n",
        "y = padding_training_sequence[:,-1]"
      ],
      "metadata": {
        "id": "uj5usMNs24nq"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class customDataset(Dataset):\n",
        "  def __init__(self,x,y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "  def __len__(self):\n",
        "    return self.x.shape[0]\n",
        "  def __getitem__(self, index):\n",
        "    return self.x[index],self.y[index]"
      ],
      "metadata": {
        "id": "yvQ2ISYf3CPj"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = customDataset(x,y)"
      ],
      "metadata": {
        "id": "R30cwIG73trl"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t2fdY-S3yFp",
        "outputId": "fb9c3e8c-7bc4-47d7-d058-ce22d42eb70b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "501"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset,batch_size=32,shuffle=True)"
      ],
      "metadata": {
        "id": "488I1LWf36DM"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMmodel(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,100)\n",
        "    self.lstm = nn.LSTM(100,150,batch_first = True)\n",
        "    self.fc = nn.Linear(150,vocab_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    embedded = self.embedding(x)\n",
        "    intermediate_hidden_layers,(final_hidden_state,final_cell_state) = self.lstm(embedded)\n",
        "    output = self.fc(final_hidden_state.squeeze(0))\n",
        "    return output"
      ],
      "metadata": {
        "id": "2XCrikjS4H1u"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMmodel(len(vocab))"
      ],
      "metadata": {
        "id": "StGCWS1o5U5r"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "GCatb-Sl5jit"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "ndoJZxUq5ai6"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEUDcWIa77Q7",
        "outputId": "7967f7f6-c49a-4086-aab4-9fc69df12135"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZQ-LH9S5nwd",
        "outputId": "0019f5a2-f603-4d4b-d6f2-53bcc5361ef2"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMmodel(\n",
              "  (embedding): Embedding(272, 100)\n",
              "  (lstm): LSTM(100, 150, batch_first=True)\n",
              "  (fc): Linear(in_features=150, out_features=272, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "L2uAV8W6515e"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "  for batch_x,batch_y in dataloader:\n",
        "    batch_x,batch_y = batch_x.to(device),batch_y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(batch_x)\n",
        "    loss = criterion(outputs,batch_y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss = total_loss + loss.item()\n",
        "  print(f\"Epoch: {epoch + 1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qddahOUm56PT",
        "outputId": "1352770e-44c2-4c00-8b4c-d17addcffadb"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 2.7007\n",
            "Epoch: 2, Loss: 2.6887\n",
            "Epoch: 3, Loss: 2.5483\n",
            "Epoch: 4, Loss: 2.5358\n",
            "Epoch: 5, Loss: 2.4752\n",
            "Epoch: 6, Loss: 2.4817\n",
            "Epoch: 7, Loss: 2.4763\n",
            "Epoch: 8, Loss: 2.4362\n",
            "Epoch: 9, Loss: 2.3429\n",
            "Epoch: 10, Loss: 2.2527\n",
            "Epoch: 11, Loss: 2.2888\n",
            "Epoch: 12, Loss: 2.2061\n",
            "Epoch: 13, Loss: 2.2675\n",
            "Epoch: 14, Loss: 2.2098\n",
            "Epoch: 15, Loss: 2.1526\n",
            "Epoch: 16, Loss: 2.2082\n",
            "Epoch: 17, Loss: 2.1114\n",
            "Epoch: 18, Loss: 2.1356\n",
            "Epoch: 19, Loss: 2.1062\n",
            "Epoch: 20, Loss: 2.0288\n",
            "Epoch: 21, Loss: 2.0618\n",
            "Epoch: 22, Loss: 1.9756\n",
            "Epoch: 23, Loss: 2.0437\n",
            "Epoch: 24, Loss: 2.0508\n",
            "Epoch: 25, Loss: 1.9748\n",
            "Epoch: 26, Loss: 1.9935\n",
            "Epoch: 27, Loss: 2.0313\n",
            "Epoch: 28, Loss: 2.0439\n",
            "Epoch: 29, Loss: 1.9501\n",
            "Epoch: 30, Loss: 1.9368\n",
            "Epoch: 31, Loss: 1.9323\n",
            "Epoch: 32, Loss: 1.8448\n",
            "Epoch: 33, Loss: 1.8368\n",
            "Epoch: 34, Loss: 1.7972\n",
            "Epoch: 35, Loss: 1.8447\n",
            "Epoch: 36, Loss: 1.7789\n",
            "Epoch: 37, Loss: 1.7934\n",
            "Epoch: 38, Loss: 1.8420\n",
            "Epoch: 39, Loss: 1.8069\n",
            "Epoch: 40, Loss: 1.7719\n",
            "Epoch: 41, Loss: 1.7600\n",
            "Epoch: 42, Loss: 1.7814\n",
            "Epoch: 43, Loss: 1.7570\n",
            "Epoch: 44, Loss: 1.7877\n",
            "Epoch: 45, Loss: 1.6810\n",
            "Epoch: 46, Loss: 1.7126\n",
            "Epoch: 47, Loss: 1.7328\n",
            "Epoch: 48, Loss: 1.7471\n",
            "Epoch: 49, Loss: 1.7459\n",
            "Epoch: 50, Loss: 1.7287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction\n",
        "\n",
        "def prediction(model, vocab, text):\n",
        "\n",
        "  # tokenize\n",
        "  tokenized_text = word_tokenize(text.lower())\n",
        "\n",
        "  # text -> numerical indices\n",
        "  numerical_text = text_to_indices(tokenized_text, vocab)\n",
        "\n",
        "  # padding\n",
        "  padded_text = torch.tensor([0] * (61 - len(numerical_text)) + numerical_text, dtype=torch.long).unsqueeze(0)\n",
        "  padded_text = padded_text.to(device)\n",
        "  # send to model\n",
        "  output = model(padded_text)\n",
        "\n",
        "  # predicted index\n",
        "  value, index = torch.max(output, dim=1)\n",
        "\n",
        "  # merge with text\n",
        "  return text + \" \" + list(vocab.keys())[index]\n"
      ],
      "metadata": {
        "id": "dL7KJGTX7Jgj"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction(model, vocab, \"Krishna was \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RW2GnjmJ7K77",
        "outputId": "d8c214f9-e06b-4d0b-d0e4-db4a1d3fc78c"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Krishna was  not'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "num_tokens = 20\n",
        "input_text = \"Krishna was \"\n",
        "\n",
        "for i in range(num_tokens):\n",
        "  output_text = prediction(model, vocab, input_text)\n",
        "  print(output_text)\n",
        "  input_text = output_text\n",
        "  time.sleep(0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kNJyChe8iDB",
        "outputId": "f1624c88-2c56-4a67-c8c5-dade8c366900"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Krishna was  not\n",
            "Krishna was  not an\n",
            "Krishna was  not an ordinary\n",
            "Krishna was  not an ordinary child\n",
            "Krishna was  not an ordinary child ,\n",
            "Krishna was  not an ordinary child , but\n",
            "Krishna was  not an ordinary child , but an\n",
            "Krishna was  not an ordinary child , but an incarnation\n",
            "Krishna was  not an ordinary child , but an incarnation of\n",
            "Krishna was  not an ordinary child , but an incarnation of the\n",
            "Krishna was  not an ordinary child , but an incarnation of the supreme\n",
            "Krishna was  not an ordinary child , but an incarnation of the supreme being\n",
            "Krishna was  not an ordinary child , but an incarnation of the supreme being being\n",
            "Krishna was  not an ordinary child , but an incarnation of the supreme being being in\n",
            "Krishna was  not an ordinary child , but an incarnation of the supreme being being in the\n",
            "Krishna was  not an ordinary child , but an incarnation of the supreme being being in the strong\n",
            "Krishna was  not an ordinary child , but an incarnation of the supreme being being in the strong devoid\n",
            "Krishna was  not an ordinary child , but an incarnation of the supreme being being in the strong devoid of\n",
            "Krishna was  not an ordinary child , but an incarnation of the supreme being being in the strong devoid of desire\n",
            "Krishna was  not an ordinary child , but an incarnation of the supreme being being in the strong devoid of desire and\n"
          ]
        }
      ]
    }
  ]
}